{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Social_Distancing_Monitor.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"5e9f16d032a241469537edb791d03f02":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_92374209198a45bc8ae418edbd030c69","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f76bf161d2e34b538de6e51658bc6edd","IPY_MODEL_98bd104b303a41b8a01c24356b45412f"]}},"92374209198a45bc8ae418edbd030c69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f76bf161d2e34b538de6e51658bc6edd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5f05815d2faf4255b45cb66a4759b4e2","_dom_classes":[],"description":"  1%","_model_name":"FloatProgressModel","bar_style":"","max":584,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":4,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4c068be181cf43b08c557076d371fd25"}},"98bd104b303a41b8a01c24356b45412f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5d38fad26706488cb1a8205ed0ed14bd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 4/584 [00:22&lt;53:39,  5.55s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_631ff44a512d448382c2dc5cb891d0f8"}},"5f05815d2faf4255b45cb66a4759b4e2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4c068be181cf43b08c557076d371fd25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5d38fad26706488cb1a8205ed0ed14bd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"631ff44a512d448382c2dc5cb891d0f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"Ygq4ZQabrDEE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595441064713,"user_tz":-330,"elapsed":2206,"user":{"displayName":"sk walia","photoUrl":"","userId":"14137336038249647208"}},"outputId":"6ae1e5dc-7c24-469f-bb34-ea1a01708397"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZdDVjFsAYmtp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595441069093,"user_tz":-330,"elapsed":2226,"user":{"displayName":"sk walia","photoUrl":"","userId":"14137336038249647208"}},"outputId":"768aaa20-f225-45a9-9fb9-6a8ec8e0f80a"},"source":["%cd '/content/drive/My Drive'"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lvrmE1UVrLQt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":372},"executionInfo":{"status":"ok","timestamp":1595435640397,"user_tz":-330,"elapsed":3732,"user":{"displayName":"sk walia","photoUrl":"","userId":"14137336038249647208"}},"outputId":"d84daab7-b74f-4c40-fa25-eb046096be1f"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Wed Jul 22 16:33:59 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 450.51.05    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   64C    P8    35W / 149W |      0MiB / 11441MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tvGhWHSHfU8B","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"status":"ok","timestamp":1595435656039,"user_tz":-330,"elapsed":9507,"user":{"displayName":"sk walia","photoUrl":"","userId":"14137336038249647208"}},"outputId":"8d573aaf-95fc-4f88-e65a-c590d38461ce"},"source":["# Install Required Libraries from PyPI\n","!pip install face-detection\n","!pip install tqdm"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: face-detection in /usr/local/lib/python3.6/dist-packages (0.1.4)\n","Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from face-detection) (1.5.1+cu101)\n","Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from face-detection) (0.6.1+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from face-detection) (1.18.5)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->face-detection) (0.16.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->face-detection) (7.0.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KgpQJHNSfsK6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595435659186,"user_tz":-330,"elapsed":5920,"user":{"displayName":"sk walia","photoUrl":"","userId":"14137336038249647208"}},"outputId":"db141c96-6d7d-4400-988e-19f57197265e"},"source":["# Import Required Libraries\n","import time\n","import math\n","import os\n","import numpy as np\n","import cv2\n","import face_detection \n","from sklearn.cluster import DBSCAN\n","from keras.models import load_model\n","from keras.applications.resnet50 import preprocess_input\n","import tqdm\n","from google.colab.patches import cv2_imshow"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"aji-NiJxFy00","colab_type":"code","colab":{}},"source":["confid = 0.5\n","thresh = 0.5\n","angle_factor = 0.8\n","H_zoom_factor = 1.2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GE1la1fzF72l","colab_type":"code","colab":{}},"source":["def dist(c1, c2):\n","    return ((c1[0] - c2[0]) ** 2 + (c1[1] - c2[1]) ** 2) ** 0.5\n","\n","def T2S(T):\n","    S = abs(T/((1+T**2)**0.5))\n","    return S\n","\n","def T2C(T):\n","    C = abs(1/((1+T**2)**0.5))\n","    return C\n","\n","def isclose(p1,p2):\n","\n","    c_d = dist(p1[2], p2[2])\n","    if(p1[1]<p2[1]):\n","        a_w = p1[0]\n","        a_h = p1[1]\n","    else:\n","        a_w = p2[0]\n","        a_h = p2[1]\n","\n","    T = 0\n","    try:\n","        T=(p2[2][1]-p1[2][1])/(p2[2][0]-p1[2][0])\n","    except ZeroDivisionError:\n","        T = 1.633123935319537e+16\n","    S = T2S(T)\n","    C = T2C(T)\n","    d_hor = C*c_d\n","    d_ver = S*c_d\n","    vc_calib_hor = a_w*1.3\n","    vc_calib_ver = a_h*0.4*angle_factor\n","    c_calib_hor = a_w *1.7\n","    c_calib_ver = a_h*0.2*angle_factor\n","    # print(p1[2], p2[2],(vc_calib_hor,d_hor),(vc_calib_ver,d_ver))\n","    if (0<d_hor<vc_calib_hor and 0<d_ver<vc_calib_ver):\n","        return 1\n","    elif 0<d_hor<c_calib_hor and 0<d_ver<c_calib_ver:\n","        return 2\n","    else:\n","        return 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7FhO0PvXF7-v","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ygki7lR_sFvf","colab_type":"code","colab":{}},"source":["# Path to the Working Environment\n","\n","# If using Google Colab (If on a Local Environment, no path required => set BASE_PATH  = \"\")\n","BASE_PATH = \"Social_Distancing_with_AI/\"\n","\n","# Path to Input Video File in the BASE_PATH\n","FILE_PATH = \"test_video.mp4\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2xrRSzoT9EBa","colab_type":"code","colab":{}},"source":["# Initialize a Face Detector \n","# Confidence Threshold can be Adjusted, Greater values would Detect only Clear Faces\n","detector = face_detection.build_detector(\"DSFDDetector\", confidence_threshold=.8, nms_iou_threshold=.4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kv-5woacC0C5","colab_type":"code","colab":{}},"source":["# Load Pretrained Face Mask Classfier (Keras Model)\n","mask_classifier = load_model(\"Social_Distancing_with_AI/Models/ResNet50_Classifier.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8O6mF_orCxN4","colab_type":"code","colab":{}},"source":["# Set the Safe Distance in Pixel Units (Minimum Distance Expected to be Maintained between People)\n","# This Parameter would Affect the Results, Adjust according to the Footage captured by CCTV Camera \n","threshold_distance = 72  # Try with different Values before Finalizing"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ejHxuy-9iZwL","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5e9f16d032a241469537edb791d03f02","92374209198a45bc8ae418edbd030c69","f76bf161d2e34b538de6e51658bc6edd","98bd104b303a41b8a01c24356b45412f","5f05815d2faf4255b45cb66a4759b4e2","4c068be181cf43b08c557076d371fd25","5d38fad26706488cb1a8205ed0ed14bd","631ff44a512d448382c2dc5cb891d0f8"],"output_embedded_package_id":"1jOY8MHSxNA7E6kaAYup09Tl-DqaCn9s3"},"outputId":"f1808ad1-3594-4932-ed0b-f41f0b89bcd4"},"source":["##################################### Analyze the Video ################################################\n","\n","# Load YOLOv3\n","net = cv2.dnn.readNet(BASE_PATH+\"Models/\"+\"yolov3-spp.weights\", BASE_PATH+\"Models/\"+\"yolov3-spp.cfg\")\n","\n","# Load COCO Classes\n","classes = []\n","with open(BASE_PATH+\"Models/\"+\"coco.names\", \"r\") as f:\n","    classes = [line.strip() for line in f.readlines()]\n","    \n","layer_names = net.getLayerNames()\n","output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n","\n","# Fetch Video Properties\n","cap = cv2.VideoCapture(BASE_PATH + FILE_PATH )\n","fps = cap.get(cv2.CAP_PROP_FPS)\n","width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n","height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n","n_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n","\n","# Create Directory for Storing Results (Make sure it doesn't already exists !)\n","os.mkdir(BASE_PATH+\"Results\")\n","os.mkdir(BASE_PATH+\"Results/Extracted_Faces\")\n","os.mkdir(BASE_PATH+\"Results/Extracted_Persons\")\n","os.mkdir(BASE_PATH+\"Results/Frames\")\n","\n","# Initialize Output Video Stream\n","out_stream = cv2.VideoWriter(\n","    BASE_PATH+'Results/test.mp4',\n","    cv2.VideoWriter_fourcc('X','V','I','D'),\n","    fps,\n","    (int(width),int(height)))\n","\n","print(\"Processing Frames :\")\n","for frame in tqdm.notebook.tqdm(range(int(n_frames))):\n","    \n","    # Capture Frame-by-Frame\n","    ret, img = cap.read()\n","\n","    # Check EOF\n","    if ret == False:\n","        break;\n","\n","    # Get Frame Dimentions\n","    height, width, channels = img.shape\n","\n","    # Detect Objects in the Frame with YOLOv3\n","    blob = cv2.dnn.blobFromImage(img, 1 / 255.0, (608, 608), swapRB=True, crop=False)\n","    net.setInput(blob)\n","    outs = net.forward(output_layers)\n","\n","    class_ids = []\n","    confidences = []\n","    boxes = []\n","    \n","    # Store Detected Objects with Labels, Bounding_Boxes and their Confidences\n","    for out in outs:\n","        for detection in out:\n","            scores = detection[5:]\n","            class_id = np.argmax(scores)\n","            confidence = scores[class_id]\n","            if confidence > 0.05:\n","                \n","                # Get Center, Height and Width of the Box\n","                center_x = int(detection[0] * width)\n","                center_y = int(detection[1] * height)\n","                w = int(detection[2] * width)\n","                h = int(detection[3] * height)\n","\n","                # Topleft Co-ordinates\n","                x = int(center_x - w / 2)\n","                y = int(center_y - h / 2)\n","\n","                boxes.append([x, y, w, h])\n","                confidences.append(float(confidence))\n","                class_ids.append(class_id)\n","\n","    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n","\n","    # Initialize empty lists for storing Bounding Boxes of People and their Faces\n","    persons = []\n","    masked_faces = []\n","    unmasked_faces = []\n","\n","    # Work on Detected Persons in the Frame\n","    for i in range(len(boxes)):\n","        if i in indexes:\n","\n","            box = np.array(boxes[i])\n","            box = np.where(box<0,0,box)\n","            (x, y, w, h) = box\n","\n","            label = str(classes[class_ids[i]])\n","\n","            if label=='person':\n","\n","                persons.append([x,y,w,h])\n","                \n","                # Save Image of Cropped Person (If not required, comment the command below)\n","                # cv2.imwrite(BASE_PATH + \"Results3/Extracted_Persons/\"+str(frame)\n","                #             +\"_\"+str(len(persons))+\".jpg\",\n","                #             img[y:y+h,x:x+w])\n","\n","                # Detect Face in the Person\n","                person_rgb = img[y:y+h,x:x+w,::-1]   # Crop & BGR to RGB\n","                detections = detector.detect(person_rgb)\n","\n","                # If a Face is Detected\n","                if detections.shape[0] > 0:\n","\n","                  detection = np.array(detections[0])\n","                  detection = np.where(detection<0,0,detection)\n","\n","                  # Calculating Co-ordinates of the Detected Face\n","                  x1 = x + int(detection[0])\n","                  x2 = x + int(detection[2])\n","                  y1 = y + int(detection[1])\n","                  y2 = y + int(detection[3])\n","\n","                  try :\n","\n","                    # Crop & BGR to RGB\n","                    face_rgb = img[y1:y2,x1:x2,::-1]   \n","\n","                    # Preprocess the Image\n","                    face_arr = cv2.resize(face_rgb, (224, 224), interpolation=cv2.INTER_NEAREST)\n","                    face_arr = np.expand_dims(face_arr, axis=0)\n","                    face_arr = preprocess_input(face_arr)\n","\n","                    # Predict if the Face is Masked or Not\n","                    score = mask_classifier.predict(face_arr)\n","\n","                    # Determine and store Results\n","                    if score[0][0]<0.10:\n","                      masked_faces.append([x1,y1,x2,y2])\n","                    else:\n","                      unmasked_faces.append([x1,y1,x2,y2])\n","\n","                    # Save Image of Cropped Face (If not required, comment the command below)\n","                    # cv2.imwrite(BASE_PATH + \"Results1/Extracted_Faces/\"+str(frame)\n","                    #             +\"_\"+str(len(persons))+\".jpg\",\n","                    #             img[y1:y2,x1:x2])\n","\n","                  except:\n","                    continue\n","    \n","    # Calculate Coordinates of People Detected and find Clusters using DBSCAN\n","    person_coordinates = []\n","\n","    for p in range(len(persons)):\n","      person_coordinates.append((persons[p][0]+int(persons[p][2]/2),persons[p][1]+int(persons[p][3]/2)))\n","\n","    clustering = DBSCAN(eps=threshold_distance,min_samples=2).fit(person_coordinates)\n","    isSafe = clustering.labels_\n","\n","    # Count \n","    person_count = len(persons)\n","    masked_face_count = len(masked_faces)\n","    unmasked_face_count = len(unmasked_faces)\n","    safe_count = np.sum((isSafe==-1)*1)\n","    unsafe_count = person_count - safe_count\n","\n","    # Show Clusters using Red Lines\n","    arg_sorted = np.argsort(isSafe)\n","\n","    for i in range(1,person_count):\n","\n","      if isSafe[arg_sorted[i]]!=-1 and isSafe[arg_sorted[i]]==isSafe[arg_sorted[i-1]]:\n","        cv2.line(img,person_coordinates[arg_sorted[i]],person_coordinates[arg_sorted[i-1]],(0,0,255),2)\n","\n","    # Put Bounding Boxes on People in the Frame\n","    for p in range(person_count):\n","\n","      a,b,c,d = persons[p]\n","\n","      # Green if Safe, Red if UnSafe\n","      if isSafe[p]==-1:\n","        cv2.rectangle(img, (a, b), (a + c, b + d), (0,255,0), 2)\n","      else:\n","        cv2.rectangle(img, (a, b), (a + c, b + d), (0,0,255), 2)\n","\n","    # Put Bounding Boxes on Faces in the Frame\n","    # Green if Safe, Red if UnSafe\n","    for f in range(masked_face_count):\n","\n","      a,b,c,d = masked_faces[f]\n","      cv2.rectangle(img, (a, b), (c,d), (0,255,0), 2)\n","\n","    for f in range(unmasked_face_count):\n","\n","      a,b,c,d = unmasked_faces[f]\n","      cv2.rectangle(img, (a, b), (c,d), (0,0,255), 2)\n","\n","    # Show Monitoring Status in a Black Box at the Top\n","    cv2.rectangle(img,(0,0),(width,50),(0,0,0),-1)\n","    cv2.rectangle(img,(1,1),(width-1,50),(255,255,255),2)\n","\n","    xpos = 15\n","\n","    string = \"Total People = \"+str(person_count)\n","    cv2.putText(img,string,(xpos,35),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n","    xpos += cv2.getTextSize(string,cv2.FONT_HERSHEY_SIMPLEX,1,2)[0][0]\n","\n","    string = \" ( \"+str(safe_count) + \" Safe \"\n","    cv2.putText(img,string,(xpos,35),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n","    xpos += cv2.getTextSize(string,cv2.FONT_HERSHEY_SIMPLEX,1,2)[0][0]\n","\n","    string = str(unsafe_count)+ \" Unsafe ) \"\n","    cv2.putText(img,string,(xpos,35),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)\n","    xpos += cv2.getTextSize(string,cv2.FONT_HERSHEY_SIMPLEX,1,2)[0][0]\n","    \n","    string = \"( \" +str(masked_face_count)+\" Masked \"+str(unmasked_face_count)+\" Unmasked \"+\\\n","             str(person_count-masked_face_count-unmasked_face_count)+\" Unknown )\"\n","    cv2.putText(img,string,(xpos,35),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,255),2)\n","\n","    # Write Frame to the Output File\n","    out_stream.write(img)\n","\n","    # Save the Frame in frame_no.png format (If not required, comment the command below)\n","    # cv2.imwrite(BASE_PATH+\"Results1/Frames/\"+str(frame)+\".jpg\",img)\n","\n","    # Use if you want to see Results Frame by Frame\n","    cv2_imshow(img)\n","\n","    # Exit on Pressing Q Key\n","    if cv2.waitKey(25) & 0xFF == ord('q'):\n","        break\n","\n","# Release Streams\n","out_stream.release()\n","cap.release()\n","cv2.destroyAllWindows()\n","\n","# Good to Go!\n","print(\"Done !\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"g5a7o3HHFVv2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1wEsROgXrbsptxp0A2xER8SzR1_fhua9a"},"outputId":"86acbc97-e43a-4ee1-afde-169626047e17"},"source":["##################################### Analyze the Video ################################################\n","\n","# Load YOLOv3\n","net = cv2.dnn.readNet(BASE_PATH+\"Models/\"+\"yolov3.weights\", BASE_PATH+\"Models/\"+\"yolov3.cfg\")\n","\n","# Load COCO Classes\n","LABELS  = []\n","with open(BASE_PATH+\"Models/\"+\"coco.names\", \"r\") as f:\n","    LABELS = [line.strip() for line in f.readlines()]\n","\n","np.random.seed(42)\n","\n","layer_names = net.getLayerNames()\n","output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n","FR=0\n","# Fetch Video Properties\n","vs = cv2.VideoCapture(BASE_PATH + FILE_PATH )\n","writer = None\n","(W, H) = (None, None)\n","\n","fl = 0\n","q = 0\n","# Create Directory for Storing Results (Make sure it doesn't already exists !)\n","\n","# Initialize Output Video Stream\n","# out_stream = cv2.VideoWriter(\n","#     BASE_PATH+'Results/test.mp4',\n","#     cv2.VideoWriter_fourcc('X','V','I','D'),\n","#     fps,\n","#     (int(width),int(height)))\n","\n","print(\"Processing Frames :\")\n","while True:\n","  \n","    # Capture Frame-by-Frame\n","    (grabbed, frame) = vs.read()\n","\n","    # Check EOF\n","    if not grabbed:\n","        break\n","\n","    if W is None or H is None:\n","        (H, W) = frame.shape[:2]\n","        FW=W\n","        if(W<1075):\n","            FW = 1075\n","        FR = np.zeros((H+210,FW,3), np.uint8)\n","\n","        col = (255,255,255)\n","        FH = H + 210\n","    FR[:] = col\n","\n","    # Detect Objects in the Frame with YOLOv3\n","    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n","    net.setInput(blob)\n","    start = time.time()\n","    outs = net.forward(output_layers)\n","    end = time.time()\n","\n","    classIDs  = []\n","    confidences = []\n","    boxes = []\n","    \n","    # Store Detected Objects with Labels, Bounding_Boxes and their Confidences\n","    for out in outs:\n","\n","        for detection in out:\n","\n","\n","            scores = detection[5:]\n","            classID = np.argmax(scores)\n","            confidence = scores[classID]\n","            if LABELS[classID] == \"person\":\n","                if confidence > confid:\n","                \n","                    # Get Center, Height and Width of the Box\n","                    box = detection[0:4] * np.array([W, H, W, H])\n","                    (centerX, centerY, width, height) = box.astype(\"int\")\n","                    x = int(centerX - (width / 2))\n","                    y = int(centerY - (height / 2))\n","                    \n","                \n","                    # Topleft Co-ordinates\n","                    # x = int(center_x - w / 2)\n","                    # y = int(center_y - h / 2)\n","\n","                    boxes.append([x, y,int(width), int(height)])\n","                    confidences.append(float(confidence))\n","                    classIDs.append(classID)\n","\n","    idxs = cv2.dnn.NMSBoxes(boxes, confidences, confid,thresh)\n","\n","    # Initialize empty lists for storing Bounding Boxes of People and their Faces\n","    \n","\n","    # Work on Detected Persons in the Frame\n","    if len(idxs) > 0:\n","        persons = []\n","        masked_faces = []\n","        unmasked_faces = []\n","        status = []\n","        idf = idxs.flatten()\n","        close_pair = []\n","        s_close_pair = []\n","        center = []\n","        co_info = []\n","\n","        for i in idf:\n","\n","            (x, y) = (boxes[i][0], boxes[i][1])\n","            (w, h) = (boxes[i][2], boxes[i][3])\n","    \n","            cen = [int(x + w / 2), int(y + h / 2)]\n","            center.append(cen)\n","            cv2.circle(frame, tuple(cen),1,(0,0,0),1)\n","            co_info.append([w, h, cen])\n","\n","            status.append(0)\n","            persons.append([x,y,w,h])\n","                \n","                # Save Image of Cropped Person (If not required, comment the command below)\n","                # cv2.imwrite(BASE_PATH + \"Results3/Extracted_Persons/\"+str(frame)\n","                #             +\"_\"+str(len(persons))+\".jpg\",\n","                #             img[y:y+h,x:x+w])\n","\n","                # Detect Face in the Person\n","            person_rgb = frame[y:y+h,x:x+w,::-1]   # Crop & BGR to RGB\n","            detections = detector.detect(person_rgb)\n","\n","                # If a Face is Detected\n","            if detections.shape[0] > 0:\n","\n","                detection = np.array(detections[0])\n","                detection = np.where(detection<0,0,detection)\n","\n","                # Calculating Co-ordinates of the Detected Face\n","                x1 = x + int(detection[0])\n","                x2 = x + int(detection[2])\n","                y1 = y + int(detection[1])\n","                y2 = y + int(detection[3])\n","\n","                try :\n","\n","\n","                    # Crop & BGR to RGB\n","                    face_rgb = frame[y1:y2,x1:x2,::-1]   \n","\n","                    # Preprocess the Image\n","                    face_arr = cv2.resize(face_rgb, (224, 224), interpolation=cv2.INTER_NEAREST)\n","                    face_arr = np.expand_dims(face_arr, axis=0)\n","                    face_arr = preprocess_input(face_arr)\n","\n","                    # Predict if the Face is Masked or Not\n","                    score = mask_classifier.predict(face_arr)\n","\n","                    # Determine and store Results\n","                    if score[0][0]<0.10:\n","                        masked_faces.append([x1,y1,x2,y2])\n","                    else:\n","                        unmasked_faces.append([x1,y1,x2,y2])\n","\n","                    # Save Image of Cropped Face (If not required, comment the command below)\n","                    # cv2.imwrite(BASE_PATH + \"Results1/Extracted_Faces/\"+str(frame)\n","                    #             +\"_\"+str(len(persons))+\".jpg\",\n","                    #             img[y1:y2,x1:x2])\n","\n","                except:\n","                    continue\n","        masked_face_count = len(masked_faces)\n","        unmasked_face_count = len(unmasked_faces)\n","        for i in range(len(center)):\n","            for j in range(len(center)):\n","    \n","                g = isclose(co_info[i],co_info[j])\n","\n","                if g == 1:\n","\n","                    close_pair.append([center[i], center[j]])\n","                    status[i] = 1\n","                    status[j] = 1\n","                elif g == 2:\n","                    s_close_pair.append([center[i], center[j]])\n","                    if status[i] != 1:\n","                        status[i] = 2\n","                    if status[j] != 1:\n","                        status[j] = 2\n","        total_p = len(center)\n","        low_risk_p = status.count(2)\n","        high_risk_p = status.count(1)\n","        safe_p = status.count(0)\n","        kk = 0                       \n","        for i in idf:\n","            cv2.line(FR,(0,H+1),(FW,H+1),(0,0,0),2)\n","            cv2.putText(FR, \"Social Distancing Analyser and Mask Monitoring wrt. COVID-19\", (210, H+60),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n","            cv2.rectangle(FR, (20, H+80), (510, H+180), (100, 100, 100), 2)\n","            cv2.putText(FR, \"Connecting lines shows closeness among people. \", (30, H+100),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (100, 100, 0), 2)\n","            cv2.putText(FR, \"-- YELLOW: CLOSE\", (50, H+90+40),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 170, 170), 2)\n","            cv2.putText(FR, \"--    RED: VERY CLOSE\", (50, H+40+110),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n","            # cv2.putText(frame, \"--    PINK: Pathway for Calibration\", (50, 150),\n","            #             cv2.FONT_HERSHEY_SIMPLEX, 0.5, (180,105,255), 1)\n","\n","            cv2.rectangle(FR, (535, H+80), (1250, H+140+40), (100, 100, 100), 2)\n","            cv2.putText(FR, \"Bounding box shows the level of risk to the person and Mask Monitoring\", (545, H+100),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (100, 100, 0), 2)\n","            cv2.putText(FR, \"-- LIGHT GREEN: SAFE\", (565,  H+90+40),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n","            cv2.putText(FR, \"-- Green: MASKED\", (865, H+90+40),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 100, 0), 2)\n","            cv2.putText(FR, \"--    DARK RED: HIGH RISK\", (565, H+150),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 150), 2)\n","            cv2.putText(FR, \"--   RED: UNMASKED\", (865, H+150),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n","            cv2.putText(FR, \"--      ORANGE: LOW RISK\", (565, H+170),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 120, 255), 2)\n","            \n","            tot_str = \"TOTAL COUNT:\" + str(total_p)\n","            high_str = \"HIGH RISK COUNT:\" + str(high_risk_p)\n","            low_str = \"LOW RISK COUNT:\" + str(low_risk_p)\n","            safe_str = \"SAFE COUNT:\" + str(safe_p)\n","            masked_str=\"MASKED COUNT:\" + str(masked_face_count)\n","            unmasked_str=\"UNMASKED COUNT:\" + str(unmasked_face_count)\n","            unknown_str=\"UNKNOWN COUNT:\" + str(total_p-masked_face_count-unmasked_face_count)\n","            cv2.putText(FR, tot_str, (1, H +25),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n","            cv2.putText(FR, safe_str, (160, H +25),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n","            cv2.putText(FR, low_str, (310, H +25),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 120, 255), 2)\n","            cv2.putText(FR, high_str, (500, H +25),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 150), 2)\n","            cv2.putText(FR, masked_str, (700, H +25),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 100, 0), 2)\n","            cv2.putText(FR, unmasked_str, (880, H +25),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n","            cv2.putText(FR, unknown_str, (1080, H +25),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n","            (x, y) = (boxes[i][0], boxes[i][1])\n","            (w, h) = (boxes[i][2], boxes[i][3])\n","            if status[kk] == 1:\n","                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 150), 2)\n","\n","            elif status[kk] == 0:\n","                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n","\n","            else:\n","                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 120, 255), 2)\n","\n","            kk += 1\n","        for h in close_pair:\n","            cv2.line(frame, tuple(h[0]), tuple(h[1]), (0, 0, 255), 2)\n","        for b in s_close_pair:\n","            cv2.line(frame, tuple(b[0]), tuple(b[1]), (0, 255, 255), 2)            \n","\n","        # Put Bounding Boxes on Faces in the Frame\n","        # Green if Safe, Red if UnSafe\n","        for f in range(masked_face_count):\n","            a,b,c,d = masked_faces[f]\n","            cv2.rectangle(frame, (a, b), (c,d), (0,100,0), 2)\n","\n","        for f in range(unmasked_face_count):\n","            a,b,c,d = unmasked_faces[f]\n","            cv2.rectangle(frame, (a, b), (c,d), (0,0,255), 2)\n","\n","        FR[0:H, 0:W] = frame  \n","        frame = FR\n","    # Write Frame to the Output File\n","    \n","    # Save the Frame in frame_no.png format (If not required, comment the command below)\n","    # cv2.imwrite(BASE_PATH+\"Results1/Frames/\"+str(frame)+\".jpg\",img)\n","\n","    # Use if you want to see Results Frame by Frame\n","        cv2_imshow(frame)\n","        cv2.waitKey(1)\n","\n","    # Exit on Pressing Q Key\n","    # if cv2.waitKey(25) & 0xFF == ord('q'):\n","    #     break\n","    if writer is None:\n","        fourcc = cv2.VideoWriter_fourcc('X','V','I','D')\n","        writer = cv2.VideoWriter(BASE_PATH+'Results/test.mp4', fourcc, 30,(frame.shape[1], frame.shape[0]), True)\n","    writer.write(frame)\n","\n","\n","# Release Streams\n","writer.release()\n","vs.release()\n","cv2.destroyAllWindows()\n","\n","# Good to Go!\n","print(\"Done !\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"S-P4TKNgeBhV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":212},"executionInfo":{"status":"ok","timestamp":1595435155828,"user_tz":-330,"elapsed":80193,"user":{"displayName":"sk walia","photoUrl":"","userId":"14137336038249647208"}},"outputId":"98ce75d6-7c98-48a3-cc0b-8c0f38c6cad9"},"source":["!wget https://pjreddie.com/media/files/yolov3-tiny.weights"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-07-22 16:24:38--  https://pjreddie.com/media/files/yolov3-tiny.weights\n","Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n","Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 35434956 (34M) [application/octet-stream]\n","Saving to: ‘yolov3-tiny.weights’\n","\n","yolov3-tiny.weights 100%[===================>]  33.79M   455KB/s    in 75s     \n","\n","2020-07-22 16:25:55 (459 KB/s) - ‘yolov3-tiny.weights’ saved [35434956/35434956]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dtqm8m5GUVUS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595441116219,"user_tz":-330,"elapsed":3294,"user":{"displayName":"sk walia","photoUrl":"","userId":"14137336038249647208"}},"outputId":"1647ed2e-bded-45c8-d3ee-2fd11fc86c84"},"source":["%cd 'Social_Distancing_with_AI'"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Social_Distancing_with_AI\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"thu5z073UXp3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595441155436,"user_tz":-330,"elapsed":9176,"user":{"displayName":"sk walia","photoUrl":"","userId":"14137336038249647208"}},"outputId":"26672cb1-bd9a-484f-fdb7-9b4aa0900db4"},"source":["!git init"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Reinitialized existing Git repository in /content/drive/My Drive/Social_Distancing_with_AI/.git/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5IoFCFDps-kr","colab_type":"code","colab":{}},"source":["!git add ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OmuWKSXltFOX","colab_type":"code","colab":{}},"source":["!git commit -m \"Project for covid19\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eJilsG3etOGf","colab_type":"code","colab":{}},"source":["!git remote add origin https://github.com/jaskirat111/Social-Distancing-Analyser-and-Mask-Monitoring-AI-system-wrt-Covid-19.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9yP_K8VMtRen","colab_type":"code","colab":{}},"source":["!git push -u origin master"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xdxtu2wsta8Y","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}